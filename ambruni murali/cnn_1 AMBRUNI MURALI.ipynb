{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge/label/cf202003 keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Flatten\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D,Convolution2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[150],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using Tensorflow, the format should be (batch, height, width, channels). If we are using Theano, the format should be (batch, channels, height, width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reshapeing our data into format of(s,h,w,c)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],28,28,1)\n",
    "X_test = X_test.reshape(X_test.shape[0],28,28,1)\n",
    "\n",
    "#feature scaling\n",
    "X_train = X_train.astype('float')\n",
    "X_test = X_test.astype('float')\n",
    "\n",
    "X_train /=255\n",
    "X_test /=255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to one-hot encode the labels i.e. Y_train and Y_test. \n",
    "\n",
    "\n",
    "Y_train[0] = [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.] since the label representated by it is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert our data into one hot encoding form\n",
    "y_train = to_categorical(y_train,10)\n",
    "y_test = to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = Sequential()  #create an object for this seq. \n",
    "\n",
    "model.add(Conv2D(32,3,3,activation='relu',input_shape=(28,28,1)))  # first conv. layer\n",
    "#no_filter,filter_size,a_f,input_shape\n",
    "\n",
    "model.add(MaxPooling2D(2,2))   #max pooling layer\n",
    "\n",
    "model.add(Conv2D(64,3,3,activation='relu'))   #second conv. layer\n",
    "\n",
    "model.add(MaxPooling2D(2,2))    #max pooling layer\n",
    "\n",
    "model.add(Dropout(0.25))     # dropout layer\n",
    "\n",
    "\n",
    "model.add(Flatten())      \n",
    "\n",
    "model.add(Dense(1000,activation='relu'))   #first hidden layer\n",
    "\n",
    "model.add(Dropout(0.5))   ## dropout layer\n",
    "\n",
    "model.add(Dense(10,activation='softmax'))  #output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*3*3+32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              1601000   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 1,629,826\n",
      "Trainable params: 1,629,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data Aug.\n",
    "##we generate random images with some manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(shear_range=.2,\n",
    "                               rotation_range=20,\n",
    "                               width_shift_range=0.2,\n",
    "                               height_shift_range=0.2,\n",
    "                               zoom_range=.2,\n",
    "                               horizontal_flip=True)\n",
    "\n",
    "test_gen = ImageDataGenerator() #validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_gen.flow(X_train, y_train, batch_size=64)\n",
    "test_generator = test_gen.flow(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using batch of 64, so the model will take 64 images at a time and train on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 or 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "938/937 [==============================] - 123s 131ms/step - loss: 0.9193 - accuracy: 0.6870 - val_loss: 0.1869 - val_accuracy: 0.9123\n",
      "Epoch 2/25\n",
      "938/937 [==============================] - 99s 106ms/step - loss: 0.4626 - accuracy: 0.8496 - val_loss: 0.0378 - val_accuracy: 0.9623\n",
      "Epoch 3/25\n",
      "938/937 [==============================] - 108s 115ms/step - loss: 0.3575 - accuracy: 0.8874 - val_loss: 0.0849 - val_accuracy: 0.9699\n",
      "Epoch 4/25\n",
      "938/937 [==============================] - 116s 124ms/step - loss: 0.3104 - accuracy: 0.9013 - val_loss: 0.0040 - val_accuracy: 0.9756\n",
      "Epoch 5/25\n",
      "938/937 [==============================] - 105s 112ms/step - loss: 0.2707 - accuracy: 0.9165 - val_loss: 0.0293 - val_accuracy: 0.9775\n",
      "Epoch 6/25\n",
      "938/937 [==============================] - 91s 97ms/step - loss: 0.2541 - accuracy: 0.9214 - val_loss: 0.0094 - val_accuracy: 0.9720\n",
      "Epoch 7/25\n",
      "938/937 [==============================] - 99s 106ms/step - loss: 0.2421 - accuracy: 0.9242 - val_loss: 0.1458 - val_accuracy: 0.9775\n",
      "Epoch 8/25\n",
      "938/937 [==============================] - 119s 127ms/step - loss: 0.2285 - accuracy: 0.9289 - val_loss: 0.0225 - val_accuracy: 0.9836\n",
      "Epoch 9/25\n",
      "938/937 [==============================] - 102s 108ms/step - loss: 0.2132 - accuracy: 0.9341 - val_loss: 0.0041 - val_accuracy: 0.9857\n",
      "Epoch 10/25\n",
      "938/937 [==============================] - 105s 112ms/step - loss: 0.2089 - accuracy: 0.9349 - val_loss: 0.0026 - val_accuracy: 0.9797\n",
      "Epoch 11/25\n",
      "938/937 [==============================] - 100s 107ms/step - loss: 0.1986 - accuracy: 0.9383 - val_loss: 3.2281e-04 - val_accuracy: 0.9858\n",
      "Epoch 12/25\n",
      "938/937 [==============================] - 103s 110ms/step - loss: 0.1942 - accuracy: 0.9392 - val_loss: 0.4332 - val_accuracy: 0.9826\n",
      "Epoch 13/25\n",
      "938/937 [==============================] - 101s 108ms/step - loss: 0.1894 - accuracy: 0.9416 - val_loss: 0.0039 - val_accuracy: 0.9833\n",
      "Epoch 14/25\n",
      "938/937 [==============================] - 112s 119ms/step - loss: 0.1862 - accuracy: 0.9415 - val_loss: 0.0089 - val_accuracy: 0.9772\n",
      "Epoch 15/25\n",
      "938/937 [==============================] - 111s 118ms/step - loss: 0.1816 - accuracy: 0.9439 - val_loss: 0.3925 - val_accuracy: 0.9808\n",
      "Epoch 16/25\n",
      "938/937 [==============================] - 100s 106ms/step - loss: 0.1776 - accuracy: 0.9445 - val_loss: 0.0311 - val_accuracy: 0.9849\n",
      "Epoch 17/25\n",
      "938/937 [==============================] - 95s 102ms/step - loss: 0.1767 - accuracy: 0.9456 - val_loss: 0.0131 - val_accuracy: 0.9836\n",
      "Epoch 18/25\n",
      "938/937 [==============================] - 95s 101ms/step - loss: 0.1756 - accuracy: 0.9463 - val_loss: 0.0313 - val_accuracy: 0.9818\n",
      "Epoch 19/25\n",
      "938/937 [==============================] - 99s 105ms/step - loss: 0.1630 - accuracy: 0.9495 - val_loss: 0.0209 - val_accuracy: 0.9846\n",
      "Epoch 20/25\n",
      "938/937 [==============================] - 127s 135ms/step - loss: 0.1673 - accuracy: 0.9481 - val_loss: 0.0072 - val_accuracy: 0.9850\n",
      "Epoch 21/25\n",
      "938/937 [==============================] - 130s 139ms/step - loss: 0.1682 - accuracy: 0.9479 - val_loss: 0.0015 - val_accuracy: 0.9857\n",
      "Epoch 22/25\n",
      "938/937 [==============================] - 144s 154ms/step - loss: 0.1580 - accuracy: 0.9519 - val_loss: 0.0080 - val_accuracy: 0.9863\n",
      "Epoch 23/25\n",
      "938/937 [==============================] - 96s 103ms/step - loss: 0.1644 - accuracy: 0.9495 - val_loss: 0.0038 - val_accuracy: 0.9845\n",
      "Epoch 24/25\n",
      "938/937 [==============================] - 121s 129ms/step - loss: 0.1599 - accuracy: 0.9511 - val_loss: 3.6392e-04 - val_accuracy: 0.9822\n",
      "Epoch 25/25\n",
      "938/937 [==============================] - 123s 131ms/step - loss: 0.1592 - accuracy: 0.9510 - val_loss: 0.0024 - val_accuracy: 0.9857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe491a0ad10>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator,steps_per_epoch=60000/64,\n",
    "                    epochs=25,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=10000/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen1=ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "                            height_shift_range=0.08, zoom_range=0.08,horizontal_flip=True)\n",
    "\n",
    "test_gen1 = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator1 = train_gen1.flow(X_train, y_train, batch_size=64)\n",
    "test_generator1 = test_gen1.flow(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "938/937 [==============================] - 119s 127ms/step - loss: 0.0732 - accuracy: 0.9779 - val_loss: 0.0404 - val_accuracy: 0.9883\n",
      "Epoch 2/25\n",
      "938/937 [==============================] - 119s 127ms/step - loss: 0.0676 - accuracy: 0.9793 - val_loss: 0.0518 - val_accuracy: 0.9906\n",
      "Epoch 3/25\n",
      "938/937 [==============================] - 110s 117ms/step - loss: 0.0645 - accuracy: 0.9800 - val_loss: 0.0026 - val_accuracy: 0.9872\n",
      "Epoch 4/25\n",
      "938/937 [==============================] - 102s 109ms/step - loss: 0.0621 - accuracy: 0.9801 - val_loss: 0.2029 - val_accuracy: 0.9893\n",
      "Epoch 5/25\n",
      "938/937 [==============================] - 110s 117ms/step - loss: 0.0610 - accuracy: 0.9806 - val_loss: 0.0802 - val_accuracy: 0.9872\n",
      "Epoch 6/25\n",
      "938/937 [==============================] - 102s 109ms/step - loss: 0.0619 - accuracy: 0.9814 - val_loss: 3.3267e-06 - val_accuracy: 0.9901\n",
      "Epoch 7/25\n",
      "938/937 [==============================] - 132s 141ms/step - loss: 0.0588 - accuracy: 0.9819 - val_loss: 2.3429e-05 - val_accuracy: 0.9880\n",
      "Epoch 8/25\n",
      "938/937 [==============================] - 136s 145ms/step - loss: 0.0571 - accuracy: 0.9821 - val_loss: 1.0646e-04 - val_accuracy: 0.9902\n",
      "Epoch 9/25\n",
      "938/937 [==============================] - 129s 137ms/step - loss: 0.0558 - accuracy: 0.9824 - val_loss: 2.0237e-04 - val_accuracy: 0.9880\n",
      "Epoch 10/25\n",
      "938/937 [==============================] - 130s 138ms/step - loss: 0.0562 - accuracy: 0.9825 - val_loss: 4.4402e-04 - val_accuracy: 0.9878\n",
      "Epoch 11/25\n",
      "938/937 [==============================] - 124s 132ms/step - loss: 0.0591 - accuracy: 0.9818 - val_loss: 6.0573e-04 - val_accuracy: 0.9864\n",
      "Epoch 12/25\n",
      "938/937 [==============================] - 121s 129ms/step - loss: 0.0579 - accuracy: 0.9820 - val_loss: 1.6866e-05 - val_accuracy: 0.9889\n",
      "Epoch 13/25\n",
      "938/937 [==============================] - 118s 126ms/step - loss: 0.0564 - accuracy: 0.9822 - val_loss: 0.0264 - val_accuracy: 0.9907\n",
      "Epoch 14/25\n",
      "938/937 [==============================] - 134s 143ms/step - loss: 0.0533 - accuracy: 0.9832 - val_loss: 5.6655e-04 - val_accuracy: 0.9898\n",
      "Epoch 15/25\n",
      "938/937 [==============================] - 120s 128ms/step - loss: 0.0522 - accuracy: 0.9835 - val_loss: 9.2577e-06 - val_accuracy: 0.9909\n",
      "Epoch 16/25\n",
      "938/937 [==============================] - 137s 147ms/step - loss: 0.0556 - accuracy: 0.9823 - val_loss: 9.4422e-05 - val_accuracy: 0.9869\n",
      "Epoch 17/25\n",
      "938/937 [==============================] - 133s 142ms/step - loss: 0.0515 - accuracy: 0.9841 - val_loss: 0.1157 - val_accuracy: 0.9912\n",
      "Epoch 18/25\n",
      "938/937 [==============================] - 129s 137ms/step - loss: 0.0507 - accuracy: 0.9841 - val_loss: 3.8200e-04 - val_accuracy: 0.9903\n",
      "Epoch 19/25\n",
      "938/937 [==============================] - 120s 127ms/step - loss: 0.0534 - accuracy: 0.9830 - val_loss: 0.0023 - val_accuracy: 0.9911\n",
      "Epoch 20/25\n",
      "938/937 [==============================] - 120s 128ms/step - loss: 0.0521 - accuracy: 0.9839 - val_loss: 0.1010 - val_accuracy: 0.9826\n",
      "Epoch 21/25\n",
      "938/937 [==============================] - 126s 134ms/step - loss: 0.0494 - accuracy: 0.9846 - val_loss: 0.0037 - val_accuracy: 0.9903\n",
      "Epoch 22/25\n",
      "938/937 [==============================] - 129s 137ms/step - loss: 0.0486 - accuracy: 0.9845 - val_loss: 2.2033e-05 - val_accuracy: 0.9910\n",
      "Epoch 23/25\n",
      "938/937 [==============================] - 119s 127ms/step - loss: 0.0485 - accuracy: 0.9848 - val_loss: 0.0750 - val_accuracy: 0.9905\n",
      "Epoch 24/25\n",
      "938/937 [==============================] - 132s 141ms/step - loss: 0.0496 - accuracy: 0.9842 - val_loss: 0.2560 - val_accuracy: 0.9879\n",
      "Epoch 25/25\n",
      "938/937 [==============================] - 115s 123ms/step - loss: 0.0514 - accuracy: 0.9839 - val_loss: 0.0443 - val_accuracy: 0.9888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe485e840d0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator1,steps_per_epoch=60000/64,epochs=25,validation_data=test_generator1,validation_steps=10000/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ghk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save_wrapper() missing 1 required positional argument: 'filepath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-d7da5d627fa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: save_wrapper() missing 1 required positional argument: 'filepath'"
     ]
    }
   ],
   "source": [
    "model.save_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnist_cnn1.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('mnist_cnn1.h5')\n",
    "#loaded_model = load_model('mnist_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "print()\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = loaded_model.evaluate(X_test, y_test)\n",
    "print()\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2    #  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread('2test.png')   #read the image ,0-gray scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape  # will be array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img,cmap='gray')\n",
    "#plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.invert(img)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.resize(img, (28, 28))   #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv2.imwrite(img,'sample.png')  #write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_classes(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.predict_classes(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
